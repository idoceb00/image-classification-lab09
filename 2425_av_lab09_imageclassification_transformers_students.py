# -*- coding: utf-8 -*-
"""Copy of 2425_AV_Lab09_ImageClassification_Transformers_students.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pbciQAZJAdD6O8PlyRBfcia3S2SRoQCI

# Image Classification with Transformers. Artificial Vision Course. Universidad de Leon

# Load CIFAR10 Dataset
"""

import numpy as np
from torchvision import datasets

def load_cifar10_dataset(samples_per_class=400, train=True, download=True, transform=None):
    """
    Loads CIFAR10 and returns either the full dataset or a subset with a fixed number of samples per class.

    Args:
        samples_per_class (int): Number of samples per class to load.
                                   For the full dataset in our lab, use 600.
                                   Default is 400.
        train (bool): If True, load the training set; otherwise the test set.
        download (bool): If True, download the dataset if not already present.
        transform (callable, optional): Transformation to apply on the images.

    Returns:
        dataset (torch.utils.data.Dataset): A (subset of) the CIFAR10 dataset.
    """
    # Load full CIFAR10 dataset
    dataset = datasets.CIFAR10(root='./data', train=train, download=download, transform=transform)

    # If samples_per_class is specified, create a subset.
    if samples_per_class is not None:
        targets = np.array(dataset.targets)
        indices = []
        for cls in np.unique(targets):
            cls_indices = np.where(targets == cls)[0]
            np.random.shuffle(cls_indices)
            selected = cls_indices[:samples_per_class]
            indices.extend(selected)
        np.random.shuffle(indices)
        from torch.utils.data import Subset
        dataset = Subset(dataset, indices)

    return dataset

# Example usage:
# full_train_dataset = load_cifar10_dataset(samples_per_class=600, train=True, transform=None)
# subset_train_dataset = load_cifar10_dataset(samples_per_class=400, train=True, transform=None)

"""# Define Data Augmentation and Preprocessing Transformations"""

from torchvision import transforms

# CIFAR10 normalization values.
cifar10_mean = (0.4914, 0.4822, 0.4465)
cifar10_std  = (0.2023, 0.1994, 0.2010)

# Training transformations: resize, robust augmentation, then normalization.
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomCrop(224, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(cifar10_mean, cifar10_std)
])

# Validation transformations: resize then normalization.
val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(cifar10_mean, cifar10_std)
])

"""# Define the Training and Evaluation Function"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import timm
import time
import matplotlib.pyplot as plt

def train_and_evaluate(model_name, train_dataset, val_dataset, num_epochs=10, batch_size=128, learning_rate=0.001, device=None):
    """
    Fine-tunes a TIMM model (pretrained on ImageNet) on CIFAR10 and evaluates it.

    Args:
        model_name (str): TIMM model name (e.g., 'deit_tiny_patch16_224').
        train_dataset (Dataset): Training dataset.
        val_dataset (Dataset): Validation dataset.
        num_epochs (int): Number of training epochs.
        batch_size (int): Batch size.
        learning_rate (float): Learning rate.
        device (torch.device, optional): Device (CPU or CUDA).

    Returns:
        model (torch.nn.Module): Trained model.
        history (dict): Training history containing epoch losses and validation accuracies.
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load model from TIMM and adjust for CIFAR10 (10 classes)
    model = timm.create_model(model_name, pretrained=True, num_classes=10)
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

    history = {'loss': [], 'val_accuracy': []}

    total_train_time = 0
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0.0
        start_epoch = time.time()

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * inputs.size(0)

        epoch_loss /= len(train_dataset)
        history['loss'].append(epoch_loss)

        elapsed_epoch = time.time() - start_epoch
        total_train_time += elapsed_epoch
        print(f"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f} - Time: {elapsed_epoch:.2f}s")

        # Evaluation on validation set
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = torch.max(outputs, dim=1)
                total += labels.size(0)
                correct += (preds == labels).sum().item()
        val_acc = 100 * correct / total
        history['val_accuracy'].append(val_acc)
        print(f"Validation Accuracy: {val_acc:.2f}%")

    print(f"Total training time: {total_train_time:.2f}s")

    # Plot training loss and validation accuracy
    epochs = range(1, num_epochs + 1)
    plt.figure(figsize=(12,5))

    plt.subplot(1,2,1)
    plt.plot(epochs, history['loss'], marker='o')
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')

    plt.subplot(1,2,2)
    plt.plot(epochs, history['val_accuracy'], marker='o', color='green')
    plt.title('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')

    plt.tight_layout()
    plt.show()

    return model, history

"""# Declare Models and Parameters

"""

# List of TIMM model names to be used.
model_names = [
    'deit_tiny_patch16_224',
    'swin_tiny_patch4_window7_224',
    'pvt_v2_b0',
    'efficientformer_l1',
    'mobilevit_s'
]

# Training parameters.
num_epochs = 10
batch_size = 128
learning_rate = 0.001
# Set the number of samples per class: use 600 for the full dataset, or 500/400 for subsets.
samples_per_class = 400  # Change this to 600 or 500 as needed.

"""# Train and Save Each Model"""

import torch
import json
import os

# Load the datasets using the transformations from Cell B.
train_dataset = load_cifar10_dataset(samples_per_class=samples_per_class, train=True, download=True, transform=train_transform)
val_dataset = load_cifar10_dataset(samples_per_class=None, train=False, download=True, transform=val_transform)  # Use full test set for evaluation.

results = {}

for model_name in model_names:
    print(f"\n--- Training model: {model_name} ---")
    start_model = time.time()
    model, history = train_and_evaluate(model_name, train_dataset, val_dataset,
                                        num_epochs=num_epochs, batch_size=batch_size,
                                        learning_rate=learning_rate)
    elapsed_model = time.time() - start_model
    print(f"Training {model_name} completed in {elapsed_model:.2f}s")

    # Save model state_dict and history.
    model_save_path = f"{model_name}_cifar10.pth"
    torch.save(model.state_dict(), model_save_path)
    history_save_path = f"{model_name}_history.json"
    with open(history_save_path, 'w') as f:
        json.dump(history, f)

    # Record results for later use.
    results[model_name] = {"history": history, "training_time": elapsed_model, "model_path": model_save_path}

# Save overall results.
with open("all_models_results.json", "w") as f:
    json.dump(results, f)

"""# Evaluate All Trained Models and Save the Evaluation Results"""

import torch
from torch.utils.data import DataLoader
import timm
import json

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
test_dataset = load_cifar10_dataset(samples_per_class=None, train=False, download=True, transform=val_transform)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

evaluation_results = {}

for model_name in model_names:
    print(f"\n--- Evaluating model: {model_name} ---")
    # Load the model architecture and weights.
    model = timm.create_model(model_name, pretrained=False, num_classes=10)
    model_path = results[model_name]["model_path"]
    model.load_state_dict(torch.load(model_path, map_location=device))
    model = model.to(device)
    model.eval()

    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, dim=1)
            total += labels.size(0)
            correct += (preds == labels).sum().item()

    test_accuracy = 100 * correct / total
    evaluation_results[model_name] = test_accuracy
    print(f"Test Accuracy for {model_name}: {test_accuracy:.2f}%")

# Save evaluation results to a file.
with open("evaluation_results.json", "w") as f:
    json.dump(evaluation_results, f)

print("\nEvaluation results:")
for model_name, acc in evaluation_results.items():
    print(f"{model_name}: {acc:.2f}%")

"""# Load, train and evaluate a single model"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
import timm
import time
import json
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
import numpy as np

# -------------------------------
# A) Load CIFAR10 Dataset Function
# -------------------------------
def load_cifar10_dataset(samples_per_class=400, train=True, download=True, transform=None):
    """
    Loads CIFAR10 and returns either the full dataset or a subset with a fixed number of samples per class.

    Args:
        samples_per_class (int): Number of samples per class to load. Use 600 for full training data.
                                   Default is 400.
        train (bool): If True, load the training set; otherwise the test set.
        download (bool): If True, download the dataset if not already present.
        transform (callable, optional): Transformation to apply on the images.

    Returns:
        dataset (torch.utils.data.Dataset): A (subset of) the CIFAR10 dataset.
    """
    dataset = datasets.CIFAR10(root='./data', train=train, download=download, transform=transform)
    if samples_per_class is not None:
        targets = np.array(dataset.targets)
        indices = []
        for cls in np.unique(targets):
            cls_indices = np.where(targets == cls)[0]
            np.random.shuffle(cls_indices)
            indices.extend(cls_indices[:samples_per_class])
        np.random.shuffle(indices)
        from torch.utils.data import Subset
        dataset = Subset(dataset, indices)
    return dataset

# -------------------------------
# B) Define Data Augmentation & Preprocessing
# -------------------------------
# CIFAR10 normalization values.
cifar10_mean = (0.4914, 0.4822, 0.4465)
cifar10_std  = (0.2023, 0.1994, 0.2010)

# Training transformation: resize (32->224), robust augmentation, then normalization.
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomCrop(224, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(cifar10_mean, cifar10_std)
])

# Validation transformation: resize and normalize.
val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(cifar10_mean, cifar10_std)
])

# -------------------------------
# C) Training and Evaluation Function
# -------------------------------
def train_and_evaluate(model_name, train_dataset, val_dataset, num_epochs=10, batch_size=128, learning_rate=0.001, device=None):
    """
    Fine-tunes a TIMM model (pretrained on ImageNet) on CIFAR10 and evaluates it.

    Args:
        model_name (str): TIMM model name (e.g., 'deit_tiny_patch16_224').
        train_dataset (Dataset): Training dataset.
        val_dataset (Dataset): Validation dataset.
        num_epochs (int): Number of training epochs.
        batch_size (int): Batch size.
        learning_rate (float): Learning rate.
        device (torch.device, optional): Device (CPU or CUDA).

    Returns:
        model (torch.nn.Module): Trained model.
        history (dict): Training history (loss and validation accuracy per epoch).
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load model from TIMM and adjust the classifier for CIFAR10 (10 classes)
    model = timm.create_model(model_name, pretrained=True, num_classes=10)
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

    history = {'loss': [], 'val_accuracy': []}
    total_training_time = 0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        start_epoch = time.time()

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)

        epoch_loss = running_loss / len(train_dataset)
        history['loss'].append(epoch_loss)
        elapsed_epoch = time.time() - start_epoch
        total_training_time += elapsed_epoch
        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Time: {elapsed_epoch:.2f}s")

        # Evaluate on validation set.
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = torch.max(outputs, dim=1)
                total += labels.size(0)
                correct += (preds == labels).sum().item()
        val_accuracy = 100 * correct / total
        history['val_accuracy'].append(val_accuracy)
        print(f"Validation Accuracy: {val_accuracy:.2f}%\n")

    print(f"Total Training Time: {total_training_time:.2f}s")

    # Plot training loss and validation accuracy.
    epochs_range = range(1, num_epochs + 1)
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, history['loss'], marker='o')
    plt.title("Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, history['val_accuracy'], marker='o', color='green')
    plt.title("Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")

    plt.tight_layout()
    plt.show()

    return model, history

# -------------------------------
# D) Declare Model to Use and Other Parameters
# -------------------------------
# For this example we use a single model from our list.
# model_name = 'deit_tiny_patch16_224'
# model_name = 'swin_tiny_patch4_window7_224'
# model_name = 'pvt_v2_b0'
# model_name = 'efficientformer_l1'
model_name = 'mobilevit_s'
# model_name = 'deit_base_patch16_224'
num_epochs = 10
batch_size = 128
learning_rate = 0.001
samples_per_class = 600  # Use 600 for the full dataset, or 500/400 for subsets.

# -------------------------------
# E) Train and Evaluate the Selected Model
# -------------------------------
# Load training and validation datasets with the corresponding transforms.
train_dataset = load_cifar10_dataset(samples_per_class=samples_per_class, train=True, download=True, transform=train_transform)
val_dataset   = load_cifar10_dataset(samples_per_class=None, train=False, download=True, transform=val_transform)  # Full test set

# Train and evaluate.
trained_model, history = train_and_evaluate(model_name, train_dataset, val_dataset,
                                            num_epochs=num_epochs, batch_size=batch_size,
                                            learning_rate=learning_rate)

# Save training history and final evaluation results.
results = {
    "model_name": model_name,
    "history": history,
    "final_val_accuracy": history['val_accuracy'][-1]
}

results_filename = f"{model_name}_results.json"
with open(results_filename, "w") as f:
    json.dump(results, f, indent=4)

print(f"Results saved to {results_filename}")

"""# List of models available in TIMM"""

import timm

def list_transformer_models():
    """Lists all model names in the TIMM library that are likely Transformer-based."""
    all_models = timm.list_models(pretrained=True)
    transformer_models = sorted([
        m for m in all_models if
        'vit' in m or 'swin' in m or 'pvt' in m or 'cvt' in m or 't2t' in m or
        'efficientformer' in m or 'mobilevit' in m or 'coatnet' in m or 'nat' in m or
        'twins' in m or 'halonet' in m or 'mvit' in m or 'clip' in m or
        'metaformer' in m or 'gfnet' in m or 'edgevit' in m or 'maxvit' in m or
        'beit' in m or 'cait' in m or 'levit' in m or 'resmlp' in m or
        'mlp_mixer' in m or 'convmixer' in m or 'jx_nest' in m or 'nest' in m or
        'poolformer' in m or 'vision_transformer' in m # Catch-all for variations
    ])
    print(f"Number of Transformer-related models in TIMM: {len(transformer_models)}")
    for model_name in transformer_models:
        print(model_name)

if __name__ == '__main__':
    list_transformer_models()